* Elaboration
Elaboration is the process of going from the surface language (SL) to
the intermediate language (IL).  The IL is then translated into the
core language (CL) of values by refinement.

The elaborator currently does two things:
1. translate from the unidirectional SL to the bidirectional IL.
2. expand wildcards in the SL to unsolved mvars in the IL.
In the future, we also expect it to insert mvars for implicit
arguments, but we don't don't implicit arguments yet.
** Elaborating wild cards
Wild cards become mvars during elaboration.  The introduced mvar
itself needs a type, which in turn introduces another mvar.  Since
everything will be skolemized, the elaborator needs to carry a
context.

In the implementation, the mvars are not pushed into the unification
state immediately, but bundled up and returned with the current
definition.  This is because we want to signal errors as early as
possible, and solving for unification variables is per definition, not
per module, or per program.  Our choice of mvar scope -- per
definition -- is arbitrary.
*** For typed binders (which we don't have)
Something like:

  elab (Lam _T b) = do
    _T'     <- elab _T
    (x , e) <- unbind b
    e'      <- extendCtx (x , _T') $ elab e
    return $ Lam _T' (bind x e')
  -- And similar for Pi and Sigma ...

  elab WildCard = do
    _T  <- fresh $ s2n "WILD_TYPE"
    w   <- fresh $ s2n "WILD"
    ctx <- getCtx
    let vs  = vars ctx
        w'  = spineApp w vs
        _T' = spineApp _T vs
    tell [ (_T , pis ctx Type) , (w ,  pis ctx _T') ]
    return w'
*** For untyped binders (which we do have)
However, unlike the Matita refinement paper, we don't have typed
lambda binders, But, neither does Gundry, so not having them means
fewer changes to his unification code.  So, this will introduce more
imprecision into our wildcard elaboration, in that we will know less
about the types, but hopefully unification will solve this just as
well ...

The simpler version is then something like:

  elab (Lam b) = do
    (x , e) <- unbind b
    e'      <- extendVars x $ elab e
    return $ Lam (bind x e')
  -- And similar for Pi and Sigma ... although now there is a
  -- distinction, since Pi and Sigma are annotated ...

  elab WildCard = do
    _T  <- fresh $ s2n "WILD_TYPE"
    w   <- fresh $ s2n "WILD"
    tell [ (_T , Type) , (w , _T) ]
    vs <- getVars
    return $ spineApp w vs

See Spire.Surface.Elaborator for the actual implementation. Pay
attention to 'elabI SWildCard' and 'elabBC'.

The Matita refiner uses the type annotations on lambda binders, but
hopefully we can recover the needed information from the bidirectional
typing ...
* Refinement
We need to bring our implementation together with Gundry's unification
implementation and the Matita refinement algorithm.  The Matita
refinement is the relatively simple part, so we can be more flexible
there.  But, it would be nice to avoid having to change the
unification code too drastically if possible.
** Examples
*** Simple type-argument inference ('id _ x')
Consider:

: id : (T:Type) -> T -> T
: A : Type
: x : A
: |-
: id (?1 A x) x => ???

resulting from the definition

: id = \A.\x. id _ x

where the "=> ???" means we want to infer a type.

Our final judgment is

: id (?1 A x) => (?1 A x) -> (?1 A x)    x <= (?1 A x)
: ----------------------------------------------------
: id (?1 A x) x => (?1 A x)

To check

: x <= (?1 A x)

we infer

: x => A

and so we learn

: A =unify= (?1 A x)

which solves for =?1= giving

: ?1 := \A.\x.A

The final judgment comes uses

: id => (A:Type) -> A -> A    (?1 A x) <= Type
: --------------------------------------------
: id (?1 A x) x => (?1 A x) -> (?1 A x)

To check

: (?1 A x) <= Type

we use

: ?1  => (A:*)(x:A) -> ?1T A x
: ?1T => (A:*)(x:A) -> Type

and so we learn that

: ?1T A x =unify= Type

which solves for =?1T= giving

: ?1T := \A.\x.Type

*** More complex type-argument inference ('id _ id _ x')
This is a hand-derived trace (really should implement a nice logger
...), corresponding to what the code was doing around [2013-12-18
Wed]. The nearest commits are spire.git/meta-vars/26c94a3 and
unify.git/master/a5defa0.

The example is
: eg : Bool -> Bool
: eg = \x -> id _ id _ x
where
: id : Pi T:*. T -> T .

Elaboration results in
: eg = \x -> id (?w1 x) id (?w2 x) x

**** Lessons learned
Studying the hand trace and ./spire output below, we notice a few
things.
***** Generation of insoluble unification problems
We generated some unification problems which are not invertible,
because the mvar not applied to a linear spine of variables.

The trouble starts when we infer the type '(?w1 x)' for 'id (?w1 x)
id', even though we've already specified
: Pi T:*. T -> T =u= (?w1 x)
The solution is to instead infer the *value* of '(?w1 x)'. Let's call
this "concretizing" (i.e. making more concrete by subbing mvar defs
for their occurrences; this is called "zonking" in GHC:
http://www.haskell.org/pipermail/glasgow-haskell-users/2013-August/024209.html).

A few places to concretize come to mind:
1. at each place we compute a canonical term, before returning it.
   E.g., we could add concretizing to the 'check' and 'infer'
   wrappers.
2. in the shape forcing functions.  E.g., we could concretize before
   the pattern match in 'forcePi'.
It seems (1) will be much less efficient than (2), and both are easy
to implement, so I'll try (2).
***** Heavy handed recursive typing of mvars
We type mvars (refine '?wT' the type of '?w') by recursively calling
'infer' and 'forcePi' for each argument, but we could do this in one
shot, which might be a lot more efficient.

For example, given '?w x1 ... xn', we infer '?w : ?wT', and then call
'forcePi ?wT' to split it into a Π-type.  We do this 'n' times, once
for each argument.  Each of the 'forcePi' calls results in a call to
the unifier on a trivial equation.

Instead, knowing that any type check of an mvar application must go
through 'infer' (because 'check' just calls 'infer' for applications),
we can special case
: infer (?w x1 ... xn)
to look up the types of the 'xi', in the context, and then fire a
single unification
: Pi x1 : T1 . ... . xn : Tn . T x1 ... xn
where the 'Ti' are 'lookupVarTy xi'.  This avoids creating 'n' new
mvars for the 'n' types 'Ti', and avoids 'n - 1' unification calls.

We then return 'T x1 ... xn' for the type of '?w x1 ... xn', and so in
the case we're inside a
: check (?w x1 ... xn) <= T
we immediate solve
: T x1 ... xn =u= T
with a second unification call.

***** Gundry code doesn't do congruence?
Running the example with ./spire produces fails with these equations
in the unification state:
: ,?Blocked (x : Bool) -> (??w13_πB_πA x (??w15 x) : Type) == (Bool : Type)
: ,?Blocked (x : Bool) -> (??w13_πB_πA x (??w15 x) : Type) == (??w15 x : Type)
and with '?w15' unsolved.  So, it appears that Gundry's code does not
do congruence, at least in this case, since the LHSs are the same, and
the derived equation
: Bool) =u= (?w15 x)
is invertible.
**** Notation
- The mvars '?w<n> : ?w<n>T : Type' are created for wildcards '_' in
  the input during the elaboration phase.
- The 'forcePi' implementation does
  : forcePi (?m es) ~> Pi xπ : mAπ . mBπ xπ
- An equality
  : T1 =u= T2
  means a unification problem sent to Gundry's unifier.

**** Relevant refinement rules, simplified
The actual rules have the form
: check a <= T ~> a'
: infer a => T ~> a'
where both rules return an output term 'a'' in the canonical language.
Here we'll pretend the input and output are equal, because evaluation
is not relevant in this example.  However, note that all types are
values.

So, we'll have
: check a <= T
: infer a => T .

The rules we care about are:

: check e <= T =
:   infer x => T'
:   T' =u= T
when there is no more specific check rule for 'e'.

: infer x => lookupInCtx x

: infer (f e) =
:   infer f => T
:   forcePi T ~> Pi xπ : Aπ . Bπ xπ
:   check e <= Aπ
:   => Bπ e

The lambda rule
: check (\x . e) <= T
:   forcePi T ~> Pi xπ : Aπ . Bπ xπ
:   extendCtx (x:Aπ) $
:     check e <= Bπ x
would also be used, but it's boring here ('T' is a concrete, given
type) so we skip it below.

**** Trace (hand calculated :P)
Assume the lambda is already processed, so that the context is
: x : Bool

And we're off!
: check (id (?w1 x) id (?w2 x) x) <= Bool
: + infer (id (?w1 x) id (?w2 x) x)
: + | infer (id (?w1 x) id (?w2 x))
: + | + infer (id (?w1 x) id)
: + | + | infer (id (?w1 x))
: + | + | + infer id => Pi T:*. T -> T
: + | + | + forcePi Pi T:*. T -> T ~>
: + | + | + check (?w1 x) <= *
: + | + | + | infer (?w1 x)
: + | + | + | + infer ?w1 => ?w1T
: + | + | + | + forcePi ?w1T ~> Pi xπ : ?w1TAπ . ?w1TBπ
: + | + | + | + check x <= ?w1TAπ
: + | + | + | + | infer x => Bool
: + | + | + | + | Bool =u= ?w1TAπ
: + | + | + | > ?w1TBπ x
: + | + | + | ?w1TBπ x =u= *
: + | + | > (?w1 x) -> (?w1 x)         // Via '(T -> T)[T |-> (?w1 x)]'.
: + | + | forcePi (?w1 x) -> (?w1 x) ~>
: + | + | check id <= (?w1 x)
: + | + | + infer id => Pi T:*. T -> T
: + | + | + Pi T:*. T -> T =u= (?w1 x) // This solves '(?w1 x)' ...
: + | + > (?w1 x)                      // ... but we don't return the solution! So,
: + | + forcePi (?w1 x) ~> Pi xπ : ?w1Aπ x . ?w1Bπ x xπ
: + | + check (?w2 x) <= ?w1Aπ x       // instead of a trivial 'forcePi', we start
: + | + | infer (?w2 x)                // introducing problems we can't solve ...
: + | + | + infer ?w2 => ?w2T
: + | + | + forcePi ?w2T ~> Pi xπ : ?w2TAπ . ?w2TBπ xπ
: + | + | + check x <= ?w2TAπ
: + | + | + | infer x => Bool
: + | + | + | Bool =u= ?w2TAπ
: + | + | > ?w2TBπ x                   // Via '[xπ |-> x]'.
: + | + | ?w2TBπ x =u= ?w1Aπ x
: + | > ?w1Bπ x (?w2 x)                // Not linear!
: + | forcePi ?w1Bπ x (?w2 x) ~> Pi xπ : ?w1BπAπ x (?w2 x) . ?w1BπBπ x (?w2 x) xπ
: + | check x <= ?w1BπAπ x (?w2 x)
: + | + infer x => Bool
: + | + Bool =u= ?w1BπAπ x (?w2 x)     // Can't solve this!
: + > ?w1BπBπ x (?w2 x) x              // Via '[xπ |-> x]'.
: + ?w1BπBπ x (?w2 x) x =u= Bool       // Can't solve this!
before finally failing due to unsolved equations.

**** Comparison with actual ./spire output
The example is
: idExample''' : Bool -> Bool
: idExample''' = \ x -> id (?w13$14 x) id (?w15$16 x) x
So, '?w13' corresponds to '?w1' above in the hand trace, and '?w15'
corresponds to '?w2' above.

The final unifier state (freshening indices elided). The second and
third blocked equations aren't generated by ./spire!
: [?w13_T_πA  : Type         := Bool
: ,?w13_T_πB  : Bool -> Type := \ _ . Type
: ,?w13_T     : Type         := Bool -> Type
: ,?w13       : Bool -> Type := \ _ . Pi (T : Type) (T -> T)
: ,?w15_T_πA  : Type         := Bool
: ,?w15_T_πB  : Bool -> Type := \ _ . Type
: ,?w15_T     : Type         := Bool -> Type
: ,?w15       : Bool -> Type
: ,?w13_πB_πA : Bool -> Type -> Type
: ,?Blocked (x : Bool) -> (??w13_πB_πA x (??w15 x) : Type) == (Bool : Type)
: ,?Blocked (x : Bool) -> (??w13_πB_πA x (??w15 x) : Type) == (??w15 x : Type)
: ,?w13_πB_πB : Pi (w13_T_πx : Bool) (Pi (w13_πx : Type) (??w13_πB_πA w13_T_πx w13_πx -> Type))
: ,?Blocked (x : Bool) -> (??w13_πB_πB x (??w15 x) x : Type) == (Bool : Type)
: ,?Blocked (x : Bool) -> (x : ??w15 x & ??w13_πB_πA x (??w15 x)) -> (??w15 x : Type) == (??w13_πB_πB x (??w15 x) x^> : Type)
: ,?w13_πA : Bool -> Type         := \ _ . Type
: ,?w13_πB : Bool -> Type -> Type := \ _ x . x -> x]
Looking at them, namely
: ,?Blocked (x : Bool) -> (??w13_πB_πA x (??w15 x) : Type) == (??w15 x : Type)
: ,?Blocked (x : Bool) -> (x : ??w15 x & ??w13_πB_πA x (??w15 x)) -> (??w15 x : Type) == (??w13_πB_πB x (??w15 x) x^> : Type)
it appears they would arise from Pi-splitting the equation
: Pi xπ : (??w13_πB_πA x (??w15 x)) . (??w13_πB_πB x (??w15 x) xπ)
: =u=
: (??w15 x) -> (??w15 x)
(up to an application of symmetry) which in turn is equivalent to
: ?w13_πB x (?w15 x)
: =u=
: (??w15 x) -> (??w15 x)
(by congruence).  But where did that come from?  It's not generated by
./spire, as far as I can tell; it doesn't appear in the logging output
below.  A clue (?): both sides are typings of
: id (?w13 x) id (?w15 x)
although I don't see how Gundry would be using that.

The unification problems sent to Gundry's unifier (freshening indices
elided):
: forcePi      : ?w13_T                  =u= (w13_T_πx : ?w13_T_πA) -> ?w13_T_πB w13_T_πx
: check'/Infer : Bool                    =u= ?w13_T_πA
: check'/Infer : ?w13_T_πB x             =u= Type
: check'/Infer : (T : Type) -> T -> T    =u= ?w13 x
: forcePi      : ?w13 x                  =u= (w13_πx : ?w13_πA x) -> ?w13_πB x w13_πx
: forcePi      : ?w15_T                  =u= (w15_T_πx : ?w15_T_πA) -> ?w15_T_πB w15_T_πx
: check'/Infer : Bool                    =u= ?w15_T_πA
: check'/Infer : ?w15_T_πB x             =u= ?w13_πA x
: forcePi      : ?w13_πB x (?w15 x)      =u= (w13_πB_πx : ?w13_πB_πA x (?w15 x)) -> ?w13_πB_πB x (?w15 x)
: check'/Infer : Bool                    =u= ?w13_πB_πA x (?w15 x)
: check'/Infer : ?w13_πB_πB x (?w15 x) x =u= Bool
These correspond exactly to the hand trace, so the mysterious
equations are being generated internally by Gundry.

** Algorithm (based on a true story)
Here we describe what our refinement algorithm would look like if we
supported lighter weight annotations.  I.e., the ability to domain
annotate lambdas and range annotate pairs, whereas right now you must
fully annotate any lambda or pair.  This is motivated by the Matita
refinement, but is more specialized, since they treat all inductive
types uniformly, whereas here we treat Sigma specially (but gain a
(trivial?) extra rule along the way).

The refinement algorithm takes a an intermediate language (IL) term
and refines it to a core language (CL) term.  In checking mode this is
done with the aid of a CL type; in inference mode this produced a CL
type.  The IL has annotated lambda domains and pair "ranges", but the
CL does not have any internal annotations. Rather, the CL has the
property that all terms -- which are necessary values -- can be
checked against their type.  So, the result of refining a module
should be thought of as a nested chain of type annotated let bindings,
although we don't describe the dependent let rule here (XXX: describe
it!).

*** Judgments
There are three judgment forms, all of which manipulate an implicit
state consisting of mvars and their solutions. The judgments are
- Type inference:
  : C |- e => T' ~> e'
  The term e is inferred to have type T' in the context C, refining to
  the term e'.

  Here e is IL, and T' and e' and all types in the context are CL.

- Type checking:
  : C |- e <= T  ~> e'
  The term e is checked to have type T in the context C, refining to
  the term e'.

  Here e is IL, and T and e' and all types in the context are CL.

- Unification
  : e1 =u= e2
  The term e1 is unified with the term e2.

  Here e1 and e2 are CL.

- Type forcing
  : T ~f> T'
  The type T is forced to have the form T'.  Here T' is pattern that
  may bind new types.

  Here T and T' are CL.

*** Rules
Because the judgments have side effects, the premises should be read
as mutating the state in order, from top to bottom.

**** Flipping the double arrow
Mode change:
: C |- e => T'
: T' =u= T''
: ----
: C |- e <= T''
The mode change rule is a last resort / default: there are special
case checking rules for some forms of e.

Annotation:
: C |- T <= Type ~> T'
: C |- e <= T' ~> e'
: ----
: C |- (e : T) => T' ~> e'
If you like, the annotation rule is another kind of mode change rule.

**** Variables
: x:T in C
: ----
: C |- x => T
There is no special case checking rule for variables.  I.e., the only
way to check a variable against a type is to infer and use the mode
change rule.

**** Pi
Note about binding: it seems I've chosen to make
: Pi x:A.B
mean that x is implicitly potentially free in B, i.e. that
: C,x:A |- B <= Type
versus B is a family indexed by A, in which case
: C |- B <= A -> Type
Need to be careful to be consistent ...

Pi term introduction:
: C |- A <= Type ~> A'
: C,x:A' |- e => B' ~> e'
: ----
: C |- \x:A.e => Pi x:A'.B' ~> \x.e'
Note that the refined lambda has no domain annotation. Practically
speaking, this choice is motivated by the fact that Gundry's
unification code does not support domain annotations.

: C |- A <= Type ~> A'
: A' =u= A''
: C,x:A'' |- e <= B'' ~> e'
: ----
: C |- \x:A.e <= Pi x:A''.B'' ~> \x.e'
(Note: is there is some question whether we should use C,x:A' or
C,x:A'' here to check the body? I thought Matita motivates one choice,
by claiming it may lead to more sensible error messages ...)

Pi term elimination:
: C |- e1 => T' ~> e1'
: T' ~f> Pi x:A'.B'
: C |- e2 <= A' ~> e2'
: ----
: C |- e1 e2 => B'[e2'/x] ~> reduce (e1' e2')
where "reduce e" means beta reduce e to normal form.  Similarly, the
substitution in B'[e2'/x] is hereditary.

Pi types:
: C |- A => Type
: C,x:A |- B
: ----
: C |- Pi x:A.B => Type

**** Sigma
Sigma term introduction:
: C |- e1 => A' ~> e1'
: C,x:A' |- B <= Type ~> B'
: C |- e2 <= B'[e1/x] ~> e'
: ----
: C |- (e1,e2:x.B) => Sigma x:A'.B' ~> (e1',e2')
Note that the refined pair has no range annotation.

: C |- e1 <= A'' ~> e1'
: C,x:A'' |- B <= Type ~> B'
: B' =u= B''
: C |- e2 <= B''[e1'/x] ~> e2'
: ----
: C |- (e1,e2:x.B) <= Sigma x:A''.B'' ~> (e1',e2')

Special case for which I don't have an analog in Pi. we can completely
infer a pair if it's first component is a variable. In some sense,
this is the only case in lambda, and here we have a free domain
annotation coming from the context:
: x:A'' in C
: C,x:A'' |- e2 => B'' ~> e2'
: C,x:A'' |- B <= Type ~> B'
: B' =u= B''
: ----
: C |- (x,e2:x.B) <= Sigma x:A''.B'' ~> (x,e2')

Sigma term elimination:
: C |- p => T' ~> p'
: C,xy:T' |- P <= Type ~> P'
: T' ~f> Sigma x:A'.B'
: C,x:A',y:B' |- b <= P'[(x,y)/xy] ~> b'
: ----
: C |- elimSigma xy.P x.y.b p => P'[p'/xy] ~> reduce (elimSigma xy.P' x.y.b' p')
(XXX: here we may be able to drop the motive xy.P' in the result,
since it corresponds to a type annotation, which we are not supposed
to need for CL values. But I need to think about this more ... The
question is, can we always check
: elimSigma x.y.b' p' <= T''
when everything is CL? Intuitively, either p' is abstract, in which
case T'' will need to explain how it interacts with the type of b', or
p' is concrete (a pair) in which case?)

Sigma types:
: C |- A => Type ~> A'
: C,x:A |- B => Type ~> B'
: ----
: C |- Sigma x:A.B => Type ~> Sigma x:A'.B'
** Implementation notes
*** MVars
I was initially inclined to bind mvars in terms as in McBride's thesis
and Brady's Idris refiner, because it seems more general and simpler
that way.  But the Matita refinement paper, and McBride and Gundry's
unification paper, bind the mvars once at the outside (in Gundry's
tests he seemingly allows any quantifier alternation, but in fact the
universal quantifier ('gal' binder) automatically skolemizes ("lifts")
any in scope existentials ('boy' binder).

So, choices include:
1. don't allow mvar binders in terms, and instead require them to all
   appear at the top level, skolemizing as necessary.

   A bonus is that we no longer have to worry about comparing mvar
   binder containing terms for equality.

2. allow mvar binders everywhere (as I'm currently) doing, but make
   the unifier fail if it encounters an mvar binder during
   unification.

The second option has the annoying side effect of needing a bunch of
erroneous default cases that we expect to never be reached. The first
option has the annoying side effect of me having to rewrite most of
the mvar code I just wrote :P

I'm going to take approach (2) now.  Specifically, I'll follow Gundry
and, unlike Matita, not have explicit mvar contexts and subs, but
rather, skolemize mvars and apply them to all their free (in scope)
term vars. In scope mvars for an mvar will be implicit.

*** Spines and beta redexes
The Matita refiner and the Gundry unifier expect spine applications. I
think it's not important to Matita -- only used for refinement of
vectors of wild cards -- but it might be important to the unifier /
non-trivial to change the unifier to use unrestricted application.
Our canonical language is in spine application form, but our source
and intermediate (expression) languages are not.  So, if the refiner
is from expression to expression or expression to canonical, then
naively, beta-redexes will have to be dealt with at some point.  But,
it might be as simple as reducing them during type checking, as in the
current canonical type checker, since in the Matita paper it appears
that all entry points to unification are between terms that are types
(and the unification routine there has a precondition that its inputs
be well typed).  The Gundry unifier of course unifies arbitrary terms
(not necessarily types), but the unification problems are type
annotated and so there may be a general well-typedness assumption
there too.

*** Source and target languages
I'm using Expression for the source and Canonical for the target.  I'm
not completely sure this will work, but it aligns with the canonical
representation used in Gundry's unification code, and the fact that
only *checked* (refined) types are used as input to unification.

In particular, this means the metavar env will use canonical types to
type the mvars it binds.
*** Scope of mvars
We want to process definitions one at a time, so we can stop as soon
as an unsolved mvar remains.  So, mvars corresponding to wild cards
should be grouped with the def they correspond to.  This includes
checking a type before checking the term it corresponds to.  Of
course, this means we won't be doing type inference, e.g. we can't
write

  id : _
  id _ a = a

as we might in Haskell (with explicit type abstraction).
*** Integrating unification into our code
To use Gundry's unification code we have at least three obvious
choices:

1. port his code to use our data types
2. port our code to use his data types
3. write back and forth translators

I'm going to start with (3) because it is the least invasive and
probably simplest to implement. It maintains the "unification as black
box" abstraction appearing in papers about refinement, and this
modularity can't hurt.  We'll have to extend the unifier later in any
case, since it has no general data type or description support, but
for now it should mostly work as is.

The interface will need to include:

- translating from Spire =Value= to Gundry =Tm=, and back.
- maintaining Spire mvar context and unification problems in a Gundry
  =Contextual= context.
* Features
** DONE Command line flags to control debug messages
Add print statements once but fire them selectively.
** TODO Logger based debug messages
Not sure what's the best way to do this, but maybe some kind of
"bracketing" and a logger. E.g.
: check e _T = do
:   let msg = <debug msg, e.g. C |- e <= _T>
:   log $ Open "check" msg
:   r <- check' e _T
:   log $ Close "check" ""
The logger decides whether or not the display the messages based on
some command line flags (e.g. --log="check infer"), and sets the
indentation level of msgs using the 'Open' and 'Close'.

E.g.

: Call 1: check
: []
: |- \x.(e1,e2)
: <= S -> T1 * T2
:   Call 1.1: check
:   [x:S]
:   |- (e1,e2)
:   <= T1 * T2
:     Call 1.1.1: check
:     [x:S]
:     |- e1
:     <= T1
:       ...
:     Call 1.1.2: check
:     [x:S]
:     |- e2
:     <= T2
:       ...

You can imagine doing weird things now, e.g. specifying that all
messages below three levels should not be shown, or only call 1.1.2
and it's sub tree should be shown. Presumably something like this
already exists...
   
** TODO Substitution unit tests
Add some!

** TODO Bring Gundry types and Spire types together
E.g., there is no unit type in the Gundry code.
** TODO Change annotation structure of lambdas and pairs?
Currently the pair and lambda are expected to be externally annotated,
but actually, all we need is RHS annotations on pairs, and domain
annotations on lambdas. Something like:

: (e1,e2:x.T2)

and

: \x:T1.e

whereas currently, the requirements are

: (e1,e2) : Sigma x:T1,T2

and

: \x.e : Pi x:T1.T2

The point is that T1 for pairs and T2 for lambdas can be inferred when
the other type is known. Note that it's not quite symmetric: we really
can't infer the general type of a pair -- there isn't one -- but we
can infer the general type of a lambda.  E.g.

: (0,0) : Sigma x:Nat. if x == 0 then Nat else Bottom
:       : Sigma x:Nat. if even x then Nat else Bottom

and those two types are not compatible.

Once we have meta vars and unification this is easy, but before that
we'll have to update all the examples with the new annotations :P
** TODO Add 'forcePi' unit tests
I had a major but inconsequential bug in the implementation for about
a month. It would have been caught by a single unit test of splitting
a var with at least two arguments :P
** TODO Build mvar types (type '?wT' for '?w') directly
We currently do this recursively in a very expensive way (simple
examples already take 1/2 a second to type check when ./spire is
compiled without optimization and 1/10 a second optimized).  See
[[*Heavy handed recursive typing of mvars]] above for details.
* Bugs
** DONE Pretty printer doesn't freshen names?
Or, the wild card to mvar implementation is buggy: the pretty printer
uses WILD as the name of all such mvars, even when there is
shadowing. Looking at the code, I see (safe) unbind is used, so I'm
confused.

Probably the bug is with the mvar impl, since this works in GHCI
(after loading Spire.Canonical.Types):

  runFreshM . replicateM 10 $ fresh (s2n "x" :: Name Int)
  ==>
  [x,x1,x2,x3,x4,x5,x6,x7,x8,x9]

But I definitely transform all wild names with 'fresh' in the
elaborator???

Solution: 'name2String' is just a projection function for extracting
the string part of the abstract type of names :P Above I used 'show'.
So, the pretty printer was wrong.
