* Elaboration
Elaboration is the process of going from the surface language (SL) to
the intermediate language (IL).  The IL is then translated into the
core language (CL) of values by refinement.

The elaborator currently does two things:
1. translate from the unidirectional SL to the bidirectional IL.
2. expand wildcards in the SL to unsolved mvars in the IL.
In the future, we also expect it to insert mvars for implicit
arguments, but we don't don't implicit arguments yet.
** Elaborating wild cards
Wild cards become mvars during elaboration.  The introduced mvar
itself needs a type, which in turn introduces another mvar.  Since
everything will be skolemized, the elaborator needs to carry a
context.
*** For typed binders (which we don't have)
Something like:

  elab (Lam _T b) = do
    _T'     <- elab _T
    (x , e) <- unbind b
    e'      <- extendCtx (x , _T') $ elab e
    return $ Lam _T' (bind x e')
  -- And similar for Pi and Sigma ...

  elab WildCard = do
    _T  <- fresh $ s2n "WILD_TYPE"
    w   <- fresh $ s2n "WILD"
    ctx <- getCtx
    let vs  = vars ctx
        w'  = spineApp w vs
        _T' = spineApp _T vs
    tell [ (_T , pis ctx Type) , (w ,  pis ctx _T') ]
    return w'
*** For untyped binders (which we do have)
However, unlike the Matita refinement paper, we don't have typed
lambda binders, But, neither does Gundry, so not having them means
fewer changes to his unification code.  So, this will introduce more
imprecision into our wildcard elaboration, in that we will know less
about the types, but hopefully unification will solve this just as
well ...

The simpler version is then something like:

  elab (Lam b) = do
    (x , e) <- unbind b
    e'      <- extendVars x $ elab e
    return $ Lam (bind x e')
  -- And similar for Pi and Sigma ... although now there is a
  -- distinction, since Pi and Sigma are annotated ...

  elab WildCard = do
    _T  <- fresh $ s2n "WILD_TYPE"
    w   <- fresh $ s2n "WILD"
    tell [ (_T , Type) , (w , _T) ]
    vs <- getVars
    return $ spineApp w vs

See Spire.Surface.Elaborator for the actual implementation. Pay
attention to 'elabI SWildCard' and 'elabBC'.

The Matita refiner uses the type annotations on lambda binders, but
hopefully we can recover the needed information from the bidirectional
typing ...
* Refinement
We need to bring our implementation together with Gundry's unification
implementation and the Matita refinement algorithm.  The Matita
refinement is the relatively simple part, so we can be more flexible
there.  But, it would be nice to avoid having to change the
unification code too drastically if possible.
** Example: Type argument inference
Consider:

: id : (T:Type) -> T -> T
: A : Type
: x : A
: |-
: id (?1 A x) x => ???

resulting from the definition

: id = \A.\x. id _ x

where the "=> ???" means we want to infer a type.

Our final judgment is

: id (?1 A x) => (?1 A x) -> (?1 A x)    x <= (?1 A x)
: ----------------------------------------------------
: id (?1 A x) x => (?1 A x)

To check

: x <= (?1 A x)

we infer

: x => A

and so we learn

: A =unify= (?1 A x)

which solves for =?1= giving

: ?1 := \A.\x.A

The final judgment comes uses

: id => (A:Type) -> A -> A    (?1 A x) <= Type
: --------------------------------------------
: id (?1 A x) x => (?1 A x) -> (?1 A x)

To check

: (?1 A x) <= Type

we use

: ?1  => (A:*)(x:A) -> ?1T A x
: ?1T => (A:*)(x:A) -> Type

and so we learn that

: ?1T A x =unify= Type

which solves for =?1T= giving

: ?1T := \A.\x.Type

** Algorithm (based on a true story)
Here we describe what our refinement algorithm would look like if we
supported lighter weight annotations.  I.e., the ability to domain
annotate lambdas and range annotate pairs, whereas right now you must
fully annotate any lambda or pair.  This is motivated by the Matita
refinement, but is more specialized, since they treat all inductive
types uniformly, whereas here we treat Sigma specially (but gain a
(trivial?) extra rule along the way).

The refinement algorithm takes a an intermediate language (IL) term
and refines it to a core language (CL) term.  In checking mode this is
done with the aid of a CL type; in inference mode this produced a CL
type.  The IL has annotated lambda domains and pair "ranges", but the
CL does not have any internal annotations. Rather, the CL has the
property that all terms -- which are necessary values -- can be
checked against their type.  So, the result of refining a module
should be thought of as a nested chain of type annotated let bindings,
although we don't describe the dependent let rule here (XXX: describe
it!).

*** Judgments
There are three judgment forms, all of which manipulate an implicit
state consisting of mvars and their solutions. The judgments are
- Type inference:
  : C |- e => T' ~> e'
  The term e is inferred to have type T' in the context C, refining to
  the term e'.

  Here e is IL, and T' and e' and all types in the context are CL.

- Type checking:
  : C |- e <= T  ~> e'
  The term e is checked to have type T in the context C, refining to
  the term e'.

  Here e is IL, and T and e' and all types in the context are CL.

- Unification
  : e1 =u= e2
  The term e1 is unified with the term e2.

  Here e1 and e2 are CL.

- Type forcing
  : T ~f> T'
  The type T is forced to have the form T'.  Here T' is pattern that
  may bind new types.

  Here T and T' are CL.

*** Rules
Because the judgments have side effects, the premises should be read
as mutating the state in order, from top to bottom.

**** Flipping the double arrow
Mode change:
: C |- e => T'
: T' =u= T''
: ----
: C |- e <= T''
The mode change rule is a last resort / default: there are special
case checking rules for some forms of e.

Annotation:
: C |- T <= Type ~> T'
: C |- e <= T' ~> e'
: ----
: C |- (e : T) => T' ~> e'
If you like, the annotation rule is another kind of mode change rule.

**** Variables
: x:T in C
: ----
: C |- x => T
There is no special case checking rule for variables.  I.e., the only
way to check a variable against a type is to infer and use the mode
change rule.

**** Pi
Note about binding: it seems I've chosen to make
: Pi x:A.B
mean that x is implicitly potentially free in B, i.e. that
: C,x:A |- B <= Type
versus B is a family indexed by A, in which case
: C |- B <= A -> Type
Need to be careful to be consistent ...

Pi term introduction:
: C |- A <= Type ~> A'
: C,x:A' |- e => B' ~> e'
: ----
: C |- \x:A.e => Pi x:A'.B' ~> \x.e'
Note that the refined lambda has no domain annotation. Practically
speaking, this choice is motivated by the fact that Gundry's
unification code does not support domain annotations.

: C |- A <= Type ~> A'
: A' =u= A''
: C,x:A'' |- e <= B'' ~> e'
: ----
: C |- \x:A.e <= Pi x:A''.B'' ~> \x.e'
(Note: is there is some question whether we should use C,x:A' or
C,x:A'' here to check the body? I thought Matita motivates one choice,
by claiming it may lead to more sensible error messages ...)

Pi term elimination:
: C |- e1 => T' ~> e1'
: T' ~f> Pi x:A'.B'
: C |- e2 <= A' ~> e2'
: ----
: C |- e1 e2 => B'[e2'/x] ~> reduce (e1' e2')
where "reduce e" means beta reduce e to normal form.  Similarly, the
substitution in B'[e2'/x] is hereditary.

Pi types:
: C |- A => Type
: C,x:A |- B
: ----
: C |- Pi x:A.B => Type

**** Sigma
Sigma term introduction:
: C |- e1 => A' ~> e1'
: C,x:A' |- B <= Type ~> B'
: C |- e2 <= B'[e1/x] ~> e'
: ----
: C |- (e1,e2:x.B) => Sigma x:A'.B' ~> (e1',e2')
Note that the refined pair has no range annotation.

: C |- e1 <= A'' ~> e1'
: C,x:A'' |- B <= Type ~> B'
: B' =u= B''
: C |- e2 <= B''[e1'/x] ~> e2'
: ----
: C |- (e1,e2:x.B) <= Sigma x:A''.B'' ~> (e1',e2')

Special case for which I don't have an analog in Pi. we can completely
infer a pair if it's first component is a variable. In some sense,
this is the only case in lambda, and here we have a free domain
annotation coming from the context:
: x:A'' in C
: C,x:A'' |- e2 => B'' ~> e2'
: C,x:A'' |- B <= Type ~> B'
: B' =u= B''
: ----
: C |- (x,e2:x.B) <= Sigma x:A''.B'' ~> (x,e2')

Sigma term elimination:
: C |- p => T' ~> p'
: C,xy:T' |- P <= Type ~> P'
: T' ~f> Sigma x:A'.B'
: C,x:A',y:B' |- b <= P'[(x,y)/xy] ~> b'
: ----
: C |- elimSigma xy.P x.y.b p => P'[p'/xy] ~> reduce (elimSigma xy.P' x.y.b' p')
(XXX: here we may be able to drop the motive xy.P' in the result,
since it corresponds to a type annotation, which we are not supposed
to need for CL values. But I need to think about this more ... The
question is, can we always check
: elimSigma x.y.b' p' <= T''
when everything is CL? Intuitively, either p' is abstract, in which
case T'' will need to explain how it interacts with the type of b', or
p' is concrete (a pair) in which case?)

Sigma types:
: C |- A => Type ~> A'
: C,x:A |- B => Type ~> B'
: ----
: C |- Sigma x:A.B => Type ~> Sigma x:A'.B'
** Implementation notes
*** MVars
I was initially inclined to bind mvars in terms as in McBride's thesis
and Brady's Idris refiner, because it seems more general and simpler
that way.  But the Matita refinement paper, and McBride and Gundry's
unification paper, bind the mvars once at the outside (in Gundry's
tests he seemingly allows any quantifier alternation, but in fact the
universal quantifier ('gal' binder) automatically skolemizes ("lifts")
any in scope existentials ('boy' binder).

So, choices include:
1. don't allow mvar binders in terms, and instead require them to all
   appear at the top level, skolemizing as necessary.

   A bonus is that we no longer have to worry about comparing mvar
   binder containing terms for equality.

2. allow mvar binders everywhere (as I'm currently) doing, but make
   the unifier fail if it encounters an mvar binder during
   unification.

The second option has the annoying side effect of needing a bunch of
erroneous default cases that we expect to never be reached. The first
option has the annoying side effect of me having to rewrite most of
the mvar code I just wrote :P

I'm going to take approach (2) now.  Specifically, I'll follow Gundry
and, unlike Matita, not have explicit mvar contexts and subs, but
rather, skolemize mvars and apply them to all their free (in scope)
term vars. In scope mvars for an mvar will be implicit.

*** Spines and beta redexes
The Matita refiner and the Gundry unifier expect spine applications. I
think it's not important to Matita -- only used for refinement of
vectors of wild cards -- but it might be important to the unifier /
non-trivial to change the unifier to use unrestricted application.
Our canonical language is in spine application form, but our source
and intermediate (expression) languages are not.  So, if the refiner
is from expression to expression or expression to canonical, then
naively, beta-redexes will have to be dealt with at some point.  But,
it might be as simple as reducing them during type checking, as in the
current canonical type checker, since in the Matita paper it appears
that all entry points to unification are between terms that are types
(and the unification routine there has a precondition that its inputs
be well typed).  The Gundry unifier of course unifies arbitrary terms
(not necessarily types), but the unification problems are type
annotated and so there may be a general well-typedness assumption
there too.

*** Source and target languages
I'm using Expression for the source and Canonical for the target.  I'm
not completely sure this will work, but it aligns with the canonical
representation used in Gundry's unification code, and the fact that
only *checked* (refined) types are used as input to unification.

In particular, this means the metavar env will use canonical types to
type the mvars it binds.
*** Scope of mvars
We want to process definitions one at a time, so we can stop as soon
as an unsolved mvar remains.  So, mvars corresponding to wild cards
should be grouped with the def they correspond to.  This includes
checking a type before checking the term it corresponds to.  Of
course, this means we won't be doing type inference, e.g. we can't
write

  id : _
  id _ a = a

as we might in Haskell (with explicit type abstraction).
*** Integrating unification into our code
To use Gundry's unification code we have at least three obvious
choices:

1. port his code to use our data types
2. port our code to use his data types
3. write back and forth translators

I'm going to start with (3) because it is the least invasive and
probably simplest to implement. It maintains the "unification as black
box" abstraction appearing in papers about refinement, and this
modularity can't hurt.  We'll have to extend the unifier later in any
case, since it has no general data type or description support, but
for now it should mostly work as is.

The interface will need to include:

- translating from Spire =Value= to Gundry =Tm=, and back.
- maintaining Spire mvar context and unification problems in a Gundry
  =Contextual= context.
* Features
** DONE Command line flags to control debug messages
Add print statements once but fire them selectively.
** TODO Logger based debug messages
Not sure what's the best way to do this, but maybe some kind of
"bracketing" and a logger. E.g.
: check e _T = do
:   let msg = <debug msg, e.g. C |- e <= _T>
:   log $ Open "check" msg
:   r <- check' e _T
:   log $ Close "check" ""
The logger decides whether or not the display the messages based on
some command line flags (e.g. --log="check infer"), and sets the
indentation level of msgs using the 'Open' and 'Close'.

E.g.

: Call 1: check
: []
: |- \x.(e1,e2)
: <= S -> T1 * T2
:   Call 1.1: check
:   [x:S]
:   |- (e1,e2)
:   <= T1 * T2
:     Call 1.1.1: check
:     [x:S]
:     |- e1
:     <= T1
:       ...
:     Call 1.1.2: check
:     [x:S]
:     |- e2
:     <= T2
:       ...

You can imagine doing weird things now, e.g. specifying that all
messages below three levels should not be shown, or only call 1.1.2
and it's sub tree should be shown. Presumably something like this
already exists...
   
** TODO Substitution unit tests
Add some!

** TODO Bring Gundry types and Spire types together
E.g., there is no unit type in the Gundry code.
** TODO Change annotation structure of lambdas and pairs?
Currently the pair and lambda are expected to be externally annotated,
but actually, all we need is RHS annotations on pairs, and domain
annotations on lambdas. Something like:

: (e1,e2:x.T2)

and

: \x:T1.e

whereas currently, the requirements are

: (e1,e2) : Sigma x:T1,T2

and

: \x.e : Pi x:T1.T2

The point is that T1 for pairs and T2 for lambdas can be inferred when
the other type is known. Note that it's not quite symmetric: we really
can't infer the general type of a pair -- there isn't one -- but we
can infer the general type of a lambda.  E.g.

: (0,0) : Sigma x:Nat. if x == 0 then Nat else Bottom
:       : Sigma x:Nat. if even x then Nat else Bottom

and those two types are not compatible.

Once we have meta vars and unification this is easy, but before that
we'll have to update all the examples with the new annotations :P
* Bugs
** DONE Pretty printer doesn't freshen names?
Or, the wild card to mvar implementation is buggy: the pretty printer
uses WILD as the name of all such mvars, even when there is
shadowing. Looking at the code, I see (safe) unbind is used, so I'm
confused.

Probably the bug is with the mvar impl, since this works in GHCI
(after loading Spire.Canonical.Types):

  runFreshM . replicateM 10 $ fresh (s2n "x" :: Name Int)
  ==>
  [x,x1,x2,x3,x4,x5,x6,x7,x8,x9]

But I definitely transform all wild names with 'fresh' in the
elaborator???

Solution: 'name2String' is just a projection function for extracting
the string part of the abstract type of names :P Above I used 'show'.
So, the pretty printer was wrong.
