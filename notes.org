* Refinement
We need to bring our implementation together with Gundry's unification
implementation and the Matita refinement algorithm.  The Matita
refinement is the relatively simple part, so we can be more flexible
there.  But, it would be nice to avoid having to change the
unification code too drastically if possible.

** MVars
I was initially inclined to bind mvars in terms as in McBride's thesis
and Brady's Idris refiner, because it seems more general and simpler
that way.  But the Matita refinement paper, and McBride and Gundry's
unification paper, bind the mvars once at the outside (in Gundry's
tests he seemingly allows any quantifier alternation, but in fact the
universal quantifier ('gal' binder) automatically skolemizes ("lifts")
any in scope existentials ('boy' binder).

So, choices include:
1. don't allow mvar binders in terms, and instead require them to all
   appear at the top level, skolemizing as necessary.

   A bonus is that we no longer have to worry about comparing mvar
   binder containing terms for equality.

2. allow mvar binders everywhere (as I'm currently) doing, but make
   the unifier fail if it encounters an mvar binder during
   unification.

The second option has the annoying side effect of needing a bunch of
erroneous default cases that we expect to never be reached. The first
option has the annoying side effect of me having to rewrite most of
the mvar code I just wrote :P

I'm going to take approach (2) now.  Specifically, I'll follow Gundry
and, unlike Matita, not have explicit mvar contexts and subs, but
rather, skolemize mvars and apply them to all their free (in scope)
term vars. In scope mvars for an mvar will be implicit.

*** Elaborating wild cards
Wild cards become mvars during elaboration.  The introduced mvar
itself needs a type, which in turn introduces another mvar.  Since
everything will be skolemized, the elaborator needs to carry a
context.

Something like:

  elab (Lam _T b) = do
    _T'     <- elab _T
    (x , e) <- unbind b
    e'      <- extendCtx (x , _T') $ elab e
    return $ Lam _T' (bind x e')
  -- And similar for Pi and Sigma ...

  elab WildCard = do
    _T  <- fresh $ s2n "WILD_TYPE"
    w   <- fresh $ s2n "WILD"
    ctx <- getCtx
    let vs  = vars ctx
        w'  = spineApp w vs
        _T' = spineApp _T vs
    tell [ (_T , pis ctx Type) , (w ,  pis ctx _T') ]
    return w'

However, unlike the Matita refinement paper, we don't have typed
lambda binders, But, neither does Gundry, so not having them means
fewer changes to his unification code.  So, this will introduce more
imprecision into our wildcard elaboration, in that we will know less
about the types, but hopefully unification will solve this just as
well ...

The simpler version is then something like:

  elab (Lam b) = do
    (x , e) <- unbind b
    e'      <- extendVars x $ elab e
    return $ Lam (bind x e')
  -- And similar for Pi and Sigma ... although now there is a
  -- distinction, since Pi and Sigma are annotated ...

  elab WildCard = do
    _T  <- fresh $ s2n "WILD_TYPE"
    w   <- fresh $ s2n "WILD"
    tell [ (_T , Type) , (w , _T) ]
    vs <- getVars
    return $ spineApp w vs

The Matita refiner uses the type annotations on lambda binders, but
hopefully we can recover the needed information from the bidirectional
typing ...

** Spines and beta redexes
The Matita refiner and the Gundry unifier expect spine applications. I
think it's not important to Matita -- only used for refinement of
vectors of wild cards -- but it might be important to the unifier /
non-trivial to change the unifier to use unrestricted application.
Our canonical language is in spine application form, but our source
and intermediate (expression) languages are not.  So, if the refiner
is from expression to expression or expression to canonical, then
naively, beta-redexes will have to be dealt with at some point.  But,
it might be as simple as reducing them during type checking, as in the
current canonical type checker, since in the Matita paper it appears
that all entry points to unification are between terms that are types
(and the unification routine there has a precondition that its inputs
be well typed).  The Gundry unifier of course unifies arbitrary terms
(not necessarily types), but the unification problems are type
annotated and so there may be a general well-typedness assumption
there too.

** Source and target languages
I'm using Expression for the source and Canonical for the target.  I'm
not completely sure this will work, but it aligns with the canonical
representation used in Gundry's unification code, and the fact that
only *checked* (refined) types are used as input to unification.

In particular, this means the metavar env will use canonical types to
type the mvars it binds.
** Scope of mvars
We want to process definitions one at a time, so we can stop as soon
as an unsolved mvar remains.  So, mvars corresponding to wild cards
should be grouped with the def they correspond to.  This includes
checking a type before checking the term it corresponds to.  Of
course, this means we won't be doing type inference, e.g. we can't
write

  id : _
  id _ a = a

as we might in Haskell (with explicit type abstraction).
** Integrating unification into our code
To use Gundry's unification code we have at least three obvious
choices:

1. port his code to use our data types
2. port our code to use his data types
3. write back and forth translators

I'm going to start with (3) because it is the least invasive and
probably simplest to implement. It maintains the "unification as black
box" abstraction appearing in papers about refinement, and this
modularity can't hurt.  We'll have to extend the unifier later in any
case, since it has no general data type or description support, but
for now it should mostly work as is.

The interface will need to include:

- translating from Spire =Value= to Gundry =Tm=, and back.
- maintaining Spire mvar context and unification problems in a Gundry
  =Contextual= context.
* Features
** TODO Command line flags to control debug messages
Add print statements once but fire them selectively.

Not sure what's the best way to do this, but maybe some kind of
"bracketing" and a logger. E.g.
: check e _T = do
:   let msg = <debug msg, e.g. C |- e <= _T>
:   log $ Open "check" msg
:   r <- check' e _T
:   log $ Close "check" ""
The logger decides whether or not the display the messages based on
some command line flags (e.g. --log="check infer"), and sets the
indentation level of msgs using the 'Open' and 'Close'.

E.g.

: Call 1: check
: []
: |- \x.(e1,e2)
: <= S -> T1 * T2
:   Call 1.1: check
:   [x:S]
:   |- (e1,e2)
:   <= T1 * T2
:     Call 1.1.1: check
:     [x:S]
:     |- e1
:     <= T1
:       ...
:     Call 1.1.2: check
:     [x:S]
:     |- e2
:     <= T2
:       ...

You can imagine doing weird things now, e.g. specifying that all
messages below three levels should not be shown, or only call 1.1.2
and it's sub tree should be shown. Presumably something like this
already exists...
   
** TODO Substitution unit tests
Add some!

* Bugs
** DONE Pretty printer doesn't freshen names?
Or, the wild card to mvar implementation is buggy: the pretty printer
uses WILD as the name of all such mvars, even when there is
shadowing. Looking at the code, I see (safe) unbind is used, so I'm
confused.

Probably the bug is with the mvar impl, since this works in GHCI
(after loading Spire.Canonical.Types):

  runFreshM . replicateM 10 $ fresh (s2n "x" :: Name Int)
  ==>
  [x,x1,x2,x3,x4,x5,x6,x7,x8,x9]

But I definitely transform all wild names with 'fresh' in the
elaborator???

Solution: 'name2String' is just a projection function for extracting
the string part of the abstract type of names :P Above I used 'show'.
So, the pretty printer was wrong.
